# Parallelism

**Parallelism** refers to techniques to make programs faster by performing several computations at the same time. This requires hardware with multiple processing units. In many cases the sub-computations are of the same structure, but this is not necessary. A key problem of parallelism is to reduce data dependencies in order to be able to perform computations on independent computation units with minimal communication between them. To this end, it can even be an advantage to do the same computation twice on different units.

## Related Terms

## Related Terms

- [concurrency](./concurrency.md)
- [thread](./thread.md)
- [multithreading](./multithreading.md)

### References

*Parallelism vs. concurrency.* (n.d.). Retrieved from https://wiki.haskell.org/Parallelism_vs._Concurrency. 